---
title: "Eye tracking Workshop CogSci F2021"
subtitle: "Data analysis exercise"
author: "Fabio Trecca"
date: "3/5/2021"
output: html_document
---

```{r setup, include=FALSE}
require(knitr)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
pacman::p_load(tidyverse, jpeg, grid, lme4, lmerTest, DHARMa, MuMIn)
```

## Load the data if necessary

```{r load data}
## We need the guess_max = Inf here, even though this will make the file loading slower
Fixations <- read_csv("/Users/manon/Desktop/Uni og Selvstudie/4. Semester/Eye Tracking Workshop/Eye-Tracking-Workhop-CogSci-F2021/data/Fixations_final.csv", guess_max = Inf)
Saccades <- read_csv("/Users/manon/Desktop/Uni og Selvstudie/4. Semester/Eye Tracking Workshop/Eye-Tracking-Workhop-CogSci-F2021/data/Saccades_final.csv")
```

## Visual foraging (Rhodes et al., 2014)

We want to test the hypothesis that eye movements are affected by task structure and goals (top-down influence): i.e., eye movements differ based on whether we are searching for the star or counting objects, *even when we are looking at the same pictures*.

### Hypothesis 1: Longer tail for saccades amplitude in Search (vs Count) condition

We know from the paper that the amplitude of saccades in the searching task has a peculiar frequency distribution with a long tail. This means that most of the saccades have very short amplitude, but few of them have very high amplitude (since our eyes are "hopping" to a new place where we can forage). The same may not be true for the counting task, where we expect there to be less variation in the amplitude of saccades (thus a shorter tail). 

Let's start by testing this hypothesis. First of all we plot the density distribution of saccade amplitudes in the two tasks:

```{r}
Saccades_forage <- subset(Saccades, Task == "Visual foraging")
Saccades_soceng <- subset(Saccades, Task == "Social engagement")
  
Saccades_search <- subset(Saccades_forage, SearchOrCount == "Search")
Saccades_count <- subset(Saccades_forage, SearchOrCount == "Count")
```

The plot seems to show that searching has a thin head and a long tail, whereas the opposite is true of counting.

Let's model the data. Start by trying to figure out how the data are distributed. We will first make a gaussian model for baseline, which we know doesn't make sense given what the data looks like. Find the fixed and random effect structure that you think is most appropriate.

```{r}
mGaus <- ...

summary(mGaus)
```

We can then try modelling the data with a lognormal distribution, which seems more appropriate to our data. (Notice that for the lognormal model to work, Amplitude should be different from zero -- please fix if necessary).

```{r}
Saccades <- Saccades %>% filter(Amplitude > 0)

mLog <- ...

summary(mLog)
```

The mean amplitude in the Search condition is significantly higher, which is consistent with the plot. But which model is the best one? Generate predictions from the models and plot their density, then compare the predictions to the distribution of the actual data. What do you notice?

```{r}
pm1 <- predict(mGaus)
pm2 <- predict(mLog)

par(mfrow=c(1,3))
plot(density(pm1), main = "Gaussian")
plot(density(pm2), main = "Log-transformed")
plot(density(Saccades$Amplitude), main = "Observed saccade amplitude")
```

Numericaly compare the model predictions to the actual data (in absolute values) in order to get an idea of how well the model predicts the data:

```{r}
summary(abs(pm1- ... ))
summary(abs(pm2- ... ))
```

We can compare observed data and model predictions more formally by looking at the residuals of the fitted models. To do this, we use the DHARMa (residual Diagnostics for HierArchical Regression Models) package:

```{r}
# first we use the simulateResiduals() function to compute the (scaled) residuals of the fitted model
# n = 250 is the number of simulations that we want DHARMa to run
dGaus <- simulateResiduals(mGaus, n = 250)
dLog <- simulateResiduals(mLog, n = 250)
```

Now we can plot the residuals for the gaussian and lognormal models and see which model does best. Do we notice any  differences? Is one model doing better than the other?

```{r}
...

```

Lastly, we can use the r.squaredGLMM() function from the MuMIn (Multi-Model Inference) package in order to calculate conditional and marginal R^2 of the two models to get a measure of their goodness of fit. Which model provides the best fit to the data?

```{r}
...

```

We can confirm these results by simply counting the number of very long saccades in the two conditions. One way of operationalize this may be to define a "Long" saccade as one that is above 2 SD from the mean, and a "Short" saccade as one that is below 2 SD from the mean. You can code "Long" as 1 and "Short" as 0.

```{r}
Saccades$LongSaccade <- ...
```

Using logistic regression, test the predictions that the probability of saccade amplitude being long is higher in the Search condition than in the Count condition:

```{r}
## Table the data first
table(
  Saccades[Saccades$Task == "Visual foraging", ]$LongSaccade,
  Saccades[Saccades$Task == "Visual foraging", ]$SearchOrCount
  )

mLogit <- ...

summary(mLogit)
```

What does this model show?

### Hypothesis 2: Longer fixation duration in Count (vs Search) condition

We can imagine that people make slightly longer fixations when counting objects rather than searching for the star. Visual foraging is usually associated with faster movements, so maybe we will see a difference here.

Let's start by plotting fixation duration in the two conditions:

```{r}
...

```

What do the plots seem to show? Do they corroborate our hypothesis?

Let's test this statistically as well. We will take the same approach as above by running a Gaussian model first, and a Log-normal model afterward.

```{r}
mGaus <- ...

mLog <- ...

summary(mGaus)
summary(mLog)
```

If the fit is singular, do a step-wise reduction of the random effect structure (random slopes first and random intercepts last if necessary).

What do the models show? And which model is the best fit to the data? Generate predictions from the models and plot their density, then compare the predictions to the distribution of the actual data.

```{r}
...

```

Again, use DHARMa and MuMIn to check which model is best:

```{r}
...

```

How do we interpret these results?

---------

## Social engagement (TylÃ©n et al., 2012)

### Hypothesis 1: Pupil size (absolute, change in pupil size across conditions) is larger in ostensive trials

We want to test the hypothesis that viewers are more emotionally engaged when involved in interaction (~direction & ostensiveness). We operationalize emotional engagement as larger pupil size.

First, we have to extract data from the PyschoPy variable "video" to determine whether the condition is Ostensive vs Non-ostensive and Direct vs Indirect:

```{r}
Fixations <- Fixations %>% 
  mutate(
    Direction = case_when(
      grepl('dir', Fixations$video) ~ "Direct",
      grepl('div', Fixations$video) ~ "Indirect"),
    Ostensiveness = case_when(
      grepl('\\+o', Fixations$video) ~ "Ostensive",
      grepl('-o', Fixations$video) ~ "Non-ostensive")
  )
```

We can now plot the data. We start by looking at the overall values of pupil size. Do they differ based on conditions (Ostensiveness & Direction)? Let's plot the data first and model them afterwards.

```{r}
...

```

Now model the data. Make sure to include all relevant fixed effects. Make also sure that you account for the fact that different people have different baselines of pupil size.

```{r}
mPupil <- ...

summary(mPupil)
```

We can then look at how pupil size changes over time. This is a good way to account for the fact that the luminosity of the stimuli changes over time.

Plot the data using geom_smooth(method = "gam") in ggplot to see how pupil size changes over time. Remember to only include data from the Social engagement task. Include both Ostensiveness and Direction as variables in the plot. What do we see?

```{r}
...

```

Now model the data to see whether we find a main effect of Ostensiveness and an interaction with Direction. Remember the time factor!

```{r}
mPupil2 <- ...

summary(mPupil2)
```

What do we see?

### Hypothesis 2: Fixations are longer when ostensive + interaction with direction

Fixations may last longer in the Ostensive + Direct condition as a sign that people stare more at faces whenever there is eye contact. In the other conditions, participants may be jumping more back and forth between face and cup, thus resulting in shorter fixations.

Let's start by plotting the data:

```{r}
...

```

The plot doesn't seem to show much difference. What if we model the data?

```{r}
mDuration <- ...

summary(mDuration)
```

What does the model show? Please interpret the result and look at whether the model is a good fit to the data. Have you taken into account the distribution of the data?s